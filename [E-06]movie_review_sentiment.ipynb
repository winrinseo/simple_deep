{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7e3696",
   "metadata": {},
   "source": [
    "## 영화 리뷰 감성분석을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aba98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225ea8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c6474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded_sentence(raw_sentence,index_to_word):\n",
    "    return ' '.join(index_to_word[idx] if idx in index_to_word else \"<UNK>\" for idx in raw_sentence[1:])\n",
    "\n",
    "def get_decoded_sentences(raw_sens,index_to_word):\n",
    "    return [get_decoded_sentence(sentence,index_to_word) for sentence in raw_sens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c98e72",
   "metadata": {},
   "source": [
    "### 기본적인 함수 준비\n",
    "* get_decoded_sentence : 인덱스로 구성된 문장을 원본 문장으로 매핑하는 함수.\n",
    "* get_decoded_sentences : 리스트로 받은 문장들을 원본 문장으로 각각 매핑해 리스트로 반환하는 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bd201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련을 위한 영화 리뷰 데이터를 다운받습니다.\n",
    "imdb = tf.keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786c435",
   "metadata": {},
   "source": [
    "* load_data() 호출 시 인자로 전달된 num_words는 단어사전에 등재할 단어의 개수로 이 개수만큼 딕셔너리까지 생성된 형태로 데이터셋이 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c1e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨 :  1\n"
     ]
    }
   ],
   "source": [
    "#다운받은 데이터를 확인해봅시다.\n",
    "print(x_train[0])\n",
    "print(\"라벨 : \",y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b9bc4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 인덱스가 뭔가요 :  the\n",
      "the는 몇번 인덱스인가요 :  1\n"
     ]
    }
   ],
   "source": [
    "#제공되는 딕셔너리를 저장합니다.\n",
    "word_to_index = imdb.get_word_index()\n",
    "#제공된 딕셔너리를 이용해 idx에서 word로 변환하는 딕셔너리를 정의합니다.\n",
    "index_to_word = {index:word for word,index in word_to_index.items()}\n",
    "print(\"1번 인덱스가 뭔가요 : \",index_to_word[1])\n",
    "print(\"the는 몇번 인덱스인가요 : \",word_to_index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9a487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
     ]
    }
   ],
   "source": [
    "#train set의 0번에는 어떤 문장이 있을지 출력해봅시다.\n",
    "print(get_decoded_sentence(x_train[0],index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d67323",
   "metadata": {},
   "source": [
    "### 그래서 이게 대체 무슨말인데?\n",
    "* 출력된 문장을 보면 의미모를 문장이 출력이 되는데 왜그럴까요?\n",
    "* 실제로 인코딩된 인덱스는 word_to_index에서 index를 기준으로 3씩 뒤로 밀려있습니다. 0~3번까지의 인덱스는 사전에 정의되어 있기 때문입니다.\n",
    "* 따라서 word_to_index는 아래처럼 보정되어야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1ad91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#매핑된 인덱스를 3씩 올려줍니다.\n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "#사전에 정의된 인덱스는 따로 처리!\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "#재정의된 word_to_index에 맞게 재정의해줍니다.\n",
    "index_to_word = {index:word for word,index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c34baaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "#보정된 딕셔너리로 번역된 문장입니다.\n",
    "print(get_decoded_sentence(x_train[0],index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10faf19c",
   "metadata": {},
   "source": [
    "* 딕셔너리를 보정하니 올바른 문장이 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a13e29",
   "metadata": {},
   "source": [
    "### 잊지맙시다.\n",
    "* 모델에 문장을 학습시켜주기 위해 keras의 pad_sequences 함수로 모든 문장의 길이를 통일시켜주어야 합니다.\n",
    "* 또, 문장의 최대길이 설정도 전체 모델 성능에 영향을 미치기에 적절한 값을 찾기위해 전체 데이터셋의 분포를 확인해 보도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21e4a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 길이의 평균 : 234.75892\n",
      "문장 길이의 최대 : 2494\n",
      "문장 길이의 표준편차 : 172.91149458735703\n",
      "전체 문장의 0.94536%가 maxlen값 이내에 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "#전체 데이터에 적용시킬 수 있을만한 maxlen값을 찾도록 하겠습니다.\n",
    "total_data_text = list(x_train)+list(x_test)\n",
    "#문장의 길이를 리스트로 저장한뒤 numpy함수를 사용하기 위해 numpy 리스트로 변환합니다.\n",
    "num_tokens = [len(token) for token in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "print(\"문장 길이의 평균 :\", np.mean(num_tokens))\n",
    "print(\"문장 길이의 최대 :\", np.max(num_tokens))\n",
    "print(\"문장 길이의 표준편차 :\", np.std(num_tokens))\n",
    "#임시로 maxlen을 평균 + 2 * 표준편차라고 한다면,\n",
    "max_tokens = np.mean(num_tokens)+2*np.std(num_tokens)\n",
    "#실수형 데이터를 정수로 바꿔줍니다.\n",
    "maxlen = int(max_tokens)\n",
    "print(\"전체 문장의 {}%가 maxlen값 이내에 포함됩니다.\".format(np.sum(num_tokens<max_tokens)/len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "222a0eda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000, 580)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1   14\n",
      "   22   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173\n",
      "   36  256    5   25  100   43  838  112   50  670    2    9   35  480\n",
      "  284    5  150    4  172  112  167    2  336  385   39    4  172 4536\n",
      " 1111   17  546   38   13  447    4  192   50   16    6  147 2025   19\n",
      "   14   22    4 1920 4613  469    4   22   71   87   12   16   43  530\n",
      "   38   76   15   13 1247    4   22   17  515   17   12   16  626   18\n",
      "    2    5   62  386   12    8  316    8  106    5    4 2223 5244   16\n",
      "  480   66 3785   33    4  130   12   16   38  619    5   25  124   51\n",
      "   36  135   48   25 1415   33    6   22   12  215   28   77   52    5\n",
      "   14  407   16   82    2    8    4  107  117 5952   15  256    4    2\n",
      "    7 3766    5  723   36   71   43  530  476   26  400  317   46    7\n",
      "    4    2 1029   13  104   88    4  381   15  297   98   32 2071   56\n",
      "   26  141    6  194 7486   18    4  226   22   21  134  476   26  480\n",
      "    5  144   30 5535   18   51   36   28  224   92   25  104    4  226\n",
      "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "   15   16 5345   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "#얻은 maxlen을 이용해 문장의 길이를 통일시킵니다.\n",
    "print(x_train.shape)\n",
    "#padding을 추가하는 위치는 post와 pre가 있습니다.\n",
    "pre_x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "pre_x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "post_x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "post_x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "print(pre_x_train.shape)\n",
    "print(pre_x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04601542",
   "metadata": {},
   "source": [
    "### 문장 길이가 통일되었습니다.\n",
    "* 길이를 늘여 빈공간에 pad문자를 넣게되는데 padding 인자를 통해 pre또는 post에 pad문자를 삽입할 수 있습니다.\n",
    "#### padding방식은 pre와 post 무엇이 더 유리할까요?\n",
    "* RNN은 입력데이터를 순차적으로 처리하며, 마지막 입력이 state에 가장 큰 영향을 미치게 됩니다. 따라서 마지막 데이터가 의미가없는 pad 문자인 것은 비효율적이며 pre가 더 유리하다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eeb596",
   "metadata": {},
   "source": [
    "### 성능을 확인해 봅시다.\n",
    "* RNN모델을 학습하여 성능을 평가해봅시다.\n",
    "* 그리고 과연 padding pre,post가 성능차이가 얼마나 나는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9355ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 훈련을 위해 데이터를 나눠줍니다.\n",
    "#검증데이터\n",
    "pre_x_val = pre_x_train[:10000]\n",
    "post_x_val = post_x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "#학습데이터\n",
    "pre_partial_x_train = pre_x_train[10000:]\n",
    "post_partial_x_train = post_x_train[10000:]\n",
    "y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4611b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "word_vector_dim = 16\n",
    "#데이터에 따라 모델을 각각 준비합니다.\n",
    "pre_model = tf.keras.Sequential()\n",
    "pre_model.add(tf.keras.layers.Embedding(vocab_size,word_vector_dim,input_shape=(None,)))\n",
    "pre_model.add(tf.keras.layers.LSTM(8))\n",
    "pre_model.add(tf.keras.layers.Dense(8,activation='relu'))\n",
    "pre_model.add(tf.keras.layers.Dense(1,activation='sigmoid'))#출력값은 0,1 둘 중 하나면 됩니다\n",
    "\n",
    "post_model = tf.keras.Sequential()\n",
    "post_model.add(tf.keras.layers.Embedding(vocab_size,word_vector_dim,input_shape=(None,)))\n",
    "post_model.add(tf.keras.layers.LSTM(8))\n",
    "post_model.add(tf.keras.layers.Dense(8,activation='relu'))\n",
    "post_model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2bd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d80ea372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 5s 45ms/step - loss: 0.6904 - accuracy: 0.5657 - val_loss: 0.6844 - val_accuracy: 0.6236\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.6640 - accuracy: 0.7163 - val_loss: 0.6348 - val_accuracy: 0.6925\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.5284 - accuracy: 0.7866 - val_loss: 0.4568 - val_accuracy: 0.8221\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.3954 - accuracy: 0.8648 - val_loss: 0.4003 - val_accuracy: 0.8497\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.3149 - accuracy: 0.8990 - val_loss: 0.3715 - val_accuracy: 0.8467\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2509 - accuracy: 0.9241 - val_loss: 0.3457 - val_accuracy: 0.8632\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2026 - accuracy: 0.9419 - val_loss: 0.3448 - val_accuracy: 0.8612\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1786 - accuracy: 0.9488 - val_loss: 0.3533 - val_accuracy: 0.8608\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1500 - accuracy: 0.9593 - val_loss: 0.3796 - val_accuracy: 0.8625\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1271 - accuracy: 0.9673 - val_loss: 0.3770 - val_accuracy: 0.8564\n"
     ]
    }
   ],
   "source": [
    "pre_model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "pre_history = pre_model.fit(pre_partial_x_train,\n",
    "                           y_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size = 512,\n",
    "                           validation_data=(pre_x_val,y_val),\n",
    "                           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "374ce82d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 2s 40ms/step - loss: 0.6931 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.4949\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.6930 - accuracy: 0.5093 - val_loss: 0.6932 - val_accuracy: 0.4968\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.6927 - accuracy: 0.5088 - val_loss: 0.6933 - val_accuracy: 0.4976\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.6921 - accuracy: 0.5139 - val_loss: 0.6927 - val_accuracy: 0.5079\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6895 - accuracy: 0.5065 - val_loss: 0.6924 - val_accuracy: 0.5021\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.6838 - accuracy: 0.5301 - val_loss: 0.6924 - val_accuracy: 0.5043\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6781 - accuracy: 0.5233 - val_loss: 0.6959 - val_accuracy: 0.5060\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6819 - accuracy: 0.5341 - val_loss: 0.6977 - val_accuracy: 0.5169\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.6779 - accuracy: 0.5234 - val_loss: 0.6948 - val_accuracy: 0.5130\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6728 - accuracy: 0.5287 - val_loss: 0.6932 - val_accuracy: 0.5095\n"
     ]
    }
   ],
   "source": [
    "post_model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "post_history = post_model.fit(post_partial_x_train,\n",
    "                           y_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size = 512,\n",
    "                           validation_data=(post_x_val,y_val),\n",
    "                           verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c59fa",
   "metadata": {},
   "source": [
    "#### 성능 평가 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bf0995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 5s - loss: 0.3933 - accuracy: 0.8510\n",
      "782/782 - 5s - loss: 0.6898 - accuracy: 0.5130\n",
      "padding의 위치에 따른 정확도 비교 -> pre : 85% post : 51%\n"
     ]
    }
   ],
   "source": [
    "pre_loss,pre_acc = pre_model.evaluate(pre_x_test,  y_test, verbose=2)\n",
    "post_loss,post_acc = post_model.evaluate(post_x_test,  y_test, verbose=2)\n",
    "print(\"padding의 위치에 따른 정확도 비교 -> pre : {}% post : {}%\".format(int(pre_acc*100),\n",
    "                                                            int(post_acc*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94787866",
   "metadata": {},
   "source": [
    "### padding 위치에 따른 성능 차이가..\n",
    "* 학습 과정을 보더라도 pre padding은 학습이 잘 되는 것을 볼 수 있지만,<br> post padding의 경우 학습이 거의 진행되지 않는것을 볼 수 있습니다.\n",
    "* post 모델도 pre_x_test로 평가하니 정확도가 70퍼센트 정도 나오긴 합니다만 여전히 pre 모델의 정확도가 우세합니다.\n",
    "* post_x_test로 평가했을땐 그저 무의미하게 선택하여 정확도가 50퍼센트가량 나오는 것을 볼 수 있습니다.\n",
    "* post의 경우 무의미한 값을 연속으로 계속 받게되어 학습이 잘 되지 않다고 생각해 볼 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ab0e9",
   "metadata": {},
   "source": [
    "### 학습한 모델의 임베딩을 확인해봅시다.\n",
    "* 임베딩 레이어는 사전의 단어 개수 X 워드 벡터 사이즈만큼의 크기를 가집니다.\n",
    "* 의미가 유사한 단어일수록 워드 벡터가 유사하겠죠?\n",
    "* 정말 그렇게 학습되었는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d229b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습한 임베딩 파라미터를 파일에 저장합니다.\n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path,'w')\n",
    "#몇개의 벡터를 얼마 사이즈로 쓸 것인지 타이틀로 기록합니다.\n",
    "f.write('{} {}\\n'.format(vocab_size-4,word_vector_dim))\n",
    "\n",
    "vectors = pre_model.get_weights()[0]\n",
    "# 첫 특수문자 4개를 제외하고 모두 파일에 기록합니다.\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i],' '.join(map(str,list(vectors[i,:])))))\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f702a",
   "metadata": {},
   "source": [
    "* gensim에서 제공하는 패키지를 이용해 파일로 된 워드 임베딩 파라미터를 읽어 word vector로 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f61a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00334141,  0.02113689,  0.0290336 ,  0.01175037, -0.03285229,\n",
       "       -0.02958348, -0.03962233,  0.03338094, -0.00960125, -0.02839767,\n",
       "       -0.03380265, -0.01193141,  0.03088752,  0.01006512,  0.03128016,\n",
       "       -0.02682097], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors as w2v\n",
    "#파일로 된 워드 임베딩을 불러옵니다.\n",
    "word_vectors = w2v.load_word2vec_format(word2vec_file_path,binary = False)\n",
    "#computer의 워드 벡터를 확인합시다.\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da59764",
   "metadata": {},
   "source": [
    "### 유사한 단어를 찾아봅시다.\n",
    "* 유사한 단어라면 워드 벡터도 비슷할텐데요, 정말로 워드 벡터가 비슷한지 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af7ddb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lang', 0.9486479759216309),\n",
       " ('mesmerizing', 0.9338877201080322),\n",
       " ('realizing', 0.9315872192382812),\n",
       " ('din', 0.9301005601882935),\n",
       " ('today', 0.9267585277557373),\n",
       " ('perfect', 0.9175127148628235),\n",
       " ('soylent', 0.9168857932090759),\n",
       " ('hooked', 0.9118993282318115),\n",
       " ('2002', 0.9108335375785828),\n",
       " ('2009', 0.90580815076828)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b8c10",
   "metadata": {},
   "source": [
    "#### 이게 love와 비슷한 단어라고?\n",
    "* 그렇게 유사하지는 않은 것 같은데 아무래도 워드 벡터의 차원도 16개밖에 안되고 훈련 데이터도 많지않아 워드 벡터가 정교하게 학습시키기 힘들었던것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a8e02",
   "metadata": {},
   "source": [
    "### 그렇다면 사전에 잘 학습된 워드 임베딩 모델은 어떨까요?\n",
    "* Google News dataset으로 학습된 Word2Vec이라는 워드 임베딩 모델을 활용해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9558e91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = w2v.load_word2vec_format(word2vec_path,binary = True,limit = 1000000)\n",
    "vector = word2vec['computer']\n",
    "#300차원의 워드 벡터입니다.\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee49bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907792091369629),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.6618633270263672),\n",
       " ('passion', 0.6100709438323975),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547305345535278),\n",
       " ('absolutely_adore', 0.5536839962005615),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80c6f5",
   "metadata": {},
   "source": [
    "* 이번엔 유사한 단어를 잘 찾은것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42dc622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "word_vector_dim = 300\n",
    "embedding_matrix = np.random.rand(vocab_size,word_vector_dim)\n",
    "#단어사전에 존재하는 단어별 워드 벡터를 복사합니다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d5edc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9abc60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))\n",
    "model.add(tf.keras.layers.LSTM(8))\n",
    "model.add(tf.keras.layers.Dense(8,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52f3b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 3s 69ms/step - loss: 0.6814 - accuracy: 0.5699 - val_loss: 0.6538 - val_accuracy: 0.6558\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.5949 - accuracy: 0.7241 - val_loss: 0.5346 - val_accuracy: 0.7526\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.4291 - accuracy: 0.8226 - val_loss: 0.4222 - val_accuracy: 0.8179\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3224 - accuracy: 0.8771 - val_loss: 0.4104 - val_accuracy: 0.8187\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2598 - accuracy: 0.9065 - val_loss: 0.3843 - val_accuracy: 0.8387\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2160 - accuracy: 0.9270 - val_loss: 0.3934 - val_accuracy: 0.8396\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.1819 - accuracy: 0.9427 - val_loss: 0.4003 - val_accuracy: 0.8388\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.1488 - accuracy: 0.9563 - val_loss: 0.4254 - val_accuracy: 0.8408\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.1263 - accuracy: 0.9650 - val_loss: 0.4447 - val_accuracy: 0.8355\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.1090 - accuracy: 0.9705 - val_loss: 0.4536 - val_accuracy: 0.8358\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10\n",
    "\n",
    "history = model.fit(pre_partial_x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(pre_x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e723f182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 5s - loss: 0.4653 - accuracy: 0.8335\n",
      "모델의 정확도는 83% 입니다.\n"
     ]
    }
   ],
   "source": [
    "model_loss,model_acc = model.evaluate(pre_x_test,y_test,verbose=2)\n",
    "print(\"모델의 정확도는 {}% 입니다.\".format(int(model_acc*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc85b0c",
   "metadata": {},
   "source": [
    "#  프로젝트 : 네이버 영화리뷰 감정분석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf42b67",
   "metadata": {},
   "source": [
    "### 데이터를 가져옵니다.\n",
    "* 데이터 다운로드 링크 https://github.com/e9t/nsmc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb1dfc06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe0e16",
   "metadata": {},
   "source": [
    "### 데이터를 정제할 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f8e614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=19000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(num_words-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead233c1",
   "metadata": {},
   "source": [
    "### 데이터의 정제 과정입니다.\n",
    "1. 원본 데이터에서 feature와 label을 분리합니다.\n",
    "2. 인덱스와 단어를 매핑할 딕셔너리를 작성합니다.\n",
    "3. 데이터에 pad문자를 넣어 길이를 맞추어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7169f110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1728/325033547.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "/tmp/ipykernel_1728/325033547.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "/tmp/ipykernel_1728/325033547.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "/tmp/ipykernel_1728/325033547.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aff3762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 67, 895, 33, 214, 15, 28, 699], [977, 481, 491, 636, 4, 110, 1554, 48, 866, 949, 11, 38, 364], [19, 192, 3]]\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(X_train[:3])\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2\n",
    "word_to_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1872ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 길이의 평균 : 13.718233430088207\n",
      "문장 길이의 최대 : 83\n",
      "문장 길이의 표준편차 : 11.469848902034261\n",
      "전체 문장의 0.9340019146202243%가 maxlen값 이내에 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "#전체 데이터에 적용시킬 수 있을만한 maxlen값을 찾도록 하겠습니다.\n",
    "total_data_text = list(X_train)+list(X_test)\n",
    "#문장의 길이를 리스트로 저장한뒤 numpy함수를 사용하기 위해 numpy 리스트로 변환합니다.\n",
    "num_tokens = [len(token) for token in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "print(\"문장 길이의 평균 :\", np.mean(num_tokens))\n",
    "print(\"문장 길이의 최대 :\", np.max(num_tokens))\n",
    "print(\"문장 길이의 표준편차 :\", np.std(num_tokens))\n",
    "#임시로 maxlen을 평균 + 2 * 표준편차라고 한다면,\n",
    "max_tokens = np.mean(num_tokens)+2*np.std(num_tokens)\n",
    "#실수형 데이터를 정수로 바꿔줍니다.\n",
    "maxlen = int(max_tokens)\n",
    "print(\"전체 문장의 {}%가 maxlen값 이내에 포함됩니다.\".format(np.sum(num_tokens<max_tokens)/len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4efd5f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  27  67 895  33 214  15  28 699]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 788 128]\n",
      "(146182, 36)\n",
      "(49157, 36)\n"
     ]
    }
   ],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09eeba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[:40000]\n",
    "y_val = y_train[:40000]\n",
    "X_train = X_train[40000:]\n",
    "y_train = y_train[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f83d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 19000\n",
    "word_vector_dim = 32\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec47b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = tf.keras.Sequential()\n",
    "first_model.add(tf.keras.layers.Embedding(vocab_size,word_vector_dim,input_shape=(None,)))\n",
    "first_model.add(tf.keras.layers.LSTM(128))\n",
    "first_model.add(tf.keras.layers.Dense(8,activation='relu'))\n",
    "first_model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "second_model = tf.keras.Sequential()\n",
    "second_model.add(tf.keras.layers.Embedding(vocab_size,word_vector_dim,input_shape=(None,)))\n",
    "second_model.add(tf.keras.layers.LSTM(128))\n",
    "second_model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "third_model = tf.keras.Sequential()\n",
    "third_model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "third_model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "third_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "third_model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "third_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f868ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "208/208 [==============================] - 4s 11ms/step - loss: 0.4643 - accuracy: 0.7691 - val_loss: 0.3670 - val_accuracy: 0.8390\n",
      "Epoch 2/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.3352 - accuracy: 0.8553 - val_loss: 0.3571 - val_accuracy: 0.8426\n",
      "Epoch 3/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.3083 - accuracy: 0.8702 - val_loss: 0.3585 - val_accuracy: 0.8419\n",
      "Epoch 4/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.2861 - accuracy: 0.8820 - val_loss: 0.3619 - val_accuracy: 0.8415\n",
      "Epoch 5/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.2650 - accuracy: 0.8915 - val_loss: 0.3843 - val_accuracy: 0.8385\n",
      "Epoch 6/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.2444 - accuracy: 0.9015 - val_loss: 0.3879 - val_accuracy: 0.8378\n",
      "Epoch 1/6\n",
      "208/208 [==============================] - 3s 10ms/step - loss: 0.4613 - accuracy: 0.7805 - val_loss: 0.3643 - val_accuracy: 0.8404\n",
      "Epoch 2/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.3343 - accuracy: 0.8566 - val_loss: 0.3554 - val_accuracy: 0.8446\n",
      "Epoch 3/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.3030 - accuracy: 0.8719 - val_loss: 0.3587 - val_accuracy: 0.8454\n",
      "Epoch 4/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.2779 - accuracy: 0.8840 - val_loss: 0.3671 - val_accuracy: 0.8446\n",
      "Epoch 5/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.2539 - accuracy: 0.8950 - val_loss: 0.3880 - val_accuracy: 0.8429\n",
      "Epoch 6/6\n",
      "208/208 [==============================] - 2s 9ms/step - loss: 0.2325 - accuracy: 0.9052 - val_loss: 0.3855 - val_accuracy: 0.8428\n",
      "Epoch 1/6\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.5807 - accuracy: 0.7544 - val_loss: 0.4423 - val_accuracy: 0.8078\n",
      "Epoch 2/6\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.3894 - accuracy: 0.8293 - val_loss: 0.3831 - val_accuracy: 0.8240\n",
      "Epoch 3/6\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.3307 - accuracy: 0.8584 - val_loss: 0.3763 - val_accuracy: 0.8293\n",
      "Epoch 4/6\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.2933 - accuracy: 0.8764 - val_loss: 0.3797 - val_accuracy: 0.8305\n",
      "Epoch 5/6\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.2628 - accuracy: 0.8915 - val_loss: 0.3912 - val_accuracy: 0.8285\n",
      "Epoch 6/6\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.2365 - accuracy: 0.9047 - val_loss: 0.4048 - val_accuracy: 0.8263\n"
     ]
    }
   ],
   "source": [
    "first_model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "first_history = first_model.fit(X_train,\n",
    "                           y_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size = 512,\n",
    "                           validation_data=(X_val,y_val),\n",
    "                           verbose = 1)\n",
    "second_model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "second_history = second_model.fit(X_train,\n",
    "                           y_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size = 512,\n",
    "                           validation_data=(X_val,y_val),\n",
    "                           verbose = 1)\n",
    "\n",
    "third_model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "third_history = third_model.fit(X_train,\n",
    "                           y_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size = 512,\n",
    "                           validation_data=(X_val,y_val),\n",
    "                           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db92507f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.3958 - accuracy: 0.8329\n",
      "모델의 정확도는 83% 입니다.\n",
      "1537/1537 - 3s - loss: 0.3934 - accuracy: 0.8387\n",
      "모델의 정확도는 83% 입니다.\n",
      "1537/1537 - 2s - loss: 0.4142 - accuracy: 0.8235\n",
      "모델의 정확도는 82% 입니다.\n"
     ]
    }
   ],
   "source": [
    "model_loss,model_acc = first_model.evaluate(X_test,y_test,verbose=2)\n",
    "print(\"모델의 정확도는 {}% 입니다.\".format(int(model_acc*100)))\n",
    "model_loss,model_acc = second_model.evaluate(X_test,y_test,verbose=2)\n",
    "print(\"모델의 정확도는 {}% 입니다.\".format(int(model_acc*100)))\n",
    "model_loss,model_acc = third_model.evaluate(X_test,y_test,verbose=2)\n",
    "print(\"모델의 정확도는 {}% 입니다.\".format(int(model_acc*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180df20",
   "metadata": {},
   "source": [
    "### 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd9f0596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = second_history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "726ce86f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d89d6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310d050",
   "metadata": {},
   "source": [
    "### 한국어 임베딩 파일을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daa17e83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/ko/ko.bin'\n",
    "\n",
    "word2vec = gensim.models.Word2Vec.load(word2vec_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b86b03f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=30185, size=200, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('열정', 0.7087529897689819),\n",
       " ('욕망', 0.670029878616333),\n",
       " ('감성', 0.6616932153701782),\n",
       " ('애정', 0.654250979423523),\n",
       " ('아름다움', 0.6380821466445923),\n",
       " ('상상력', 0.6374208331108093),\n",
       " ('슬픔', 0.6216992139816284),\n",
       " ('애국심', 0.6152282953262329),\n",
       " ('독창성', 0.610627293586731),\n",
       " ('기쁨', 0.6095349788665771)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word2vec)\n",
    "word2vec.wv.most_similar(\"정열\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4e8d3",
   "metadata": {},
   "source": [
    "* 유사한 단어끼리 아주 잘묶여있는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d72b15",
   "metadata": {},
   "source": [
    "### 이하 과정은 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd8329eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1728/325033547.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "/tmp/ipykernel_1728/325033547.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "/tmp/ipykernel_1728/325033547.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "/tmp/ipykernel_1728/325033547.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test,word_to_index = load_data(train_data,test_data,25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26d28ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2\n",
    "word_to_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2679a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word,index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3b682f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 길이의 평균 : 13.718233430088207\n",
      "문장 길이의 최대 : 83\n",
      "문장 길이의 표준편차 : 11.469848902034261\n",
      "전체 문장의 0.9340019146202243%가 maxlen값 이내에 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "#전체 데이터에 적용시킬 수 있을만한 maxlen값을 찾도록 하겠습니다.\n",
    "total_data_text = list(X_train)+list(X_test)\n",
    "#문장의 길이를 리스트로 저장한뒤 numpy함수를 사용하기 위해 numpy 리스트로 변환합니다.\n",
    "num_tokens = [len(token) for token in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "print(\"문장 길이의 평균 :\", np.mean(num_tokens))\n",
    "print(\"문장 길이의 최대 :\", np.max(num_tokens))\n",
    "print(\"문장 길이의 표준편차 :\", np.std(num_tokens))\n",
    "#임시로 maxlen을 평균 + 2 * 표준편차라고 한다면,\n",
    "max_tokens = np.mean(num_tokens)+2*np.std(num_tokens)\n",
    "#실수형 데이터를 정수로 바꿔줍니다.\n",
    "maxlen = int(max_tokens)\n",
    "print(\"전체 문장의 {}%가 maxlen값 이내에 포함됩니다.\".format(np.sum(num_tokens<max_tokens)/len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79653805",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                        value = word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                       maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35029877",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[:30000]\n",
    "y_val = y_train[:30000]\n",
    "X_train = X_train[30000:]\n",
    "y_train = y_train[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0766dddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화\n",
      "[-1.75778365 -1.08742785  1.53008664 -0.1115231  -0.37980682  1.48285174\n",
      "  1.31804192  0.11094163  0.7430535  -0.45461136  0.58841336  0.57639128\n",
      "  1.21070695  1.31327951 -0.86962503 -0.18507595 -0.47440064  1.51007247\n",
      "  1.09657943  1.06008232 -0.27457932 -0.70003706  2.31175113  1.49448836\n",
      "  0.25560892 -2.86665893 -0.28312334  0.34263936 -0.67723423  0.71714777\n",
      "  0.25549442  0.71732044 -0.13262457  0.01792452 -0.31847739  0.5271619\n",
      "  0.7561084  -2.12470651  1.06142902 -0.21065854  0.68773431 -1.49563825\n",
      "  0.60346967 -2.6955893   0.37694618 -1.01641846  0.54306632  0.1200121\n",
      " -2.63157177  0.62167418  1.15839756 -2.53859615  1.32631195 -0.10284371\n",
      " -0.0286147  -0.91329467  0.76475638  0.79202783 -1.86259568 -0.74183953\n",
      "  0.58842772 -0.99179918 -0.62114453  1.53678155 -0.66289389  0.67121029\n",
      "  0.12914915  0.21228492  0.90176553 -0.25083402  0.71500814  0.08644514\n",
      "  0.59993285  0.57661372  0.64095974  0.47888306 -2.84262133 -2.85026813\n",
      " -0.140544   -1.59173644  0.26691505  0.59476066  0.85868204  1.03223515\n",
      "  0.25671318 -0.34831643  1.75292695 -0.21967097 -0.77352476  1.6995213\n",
      "  1.39964914 -0.94198358  0.85996443 -1.88128757 -2.54286051  0.39351937\n",
      " -1.28828049  0.56548136  1.00627303  1.22175848  3.57447934  1.71773696\n",
      "  1.69171584 -2.21769047 -0.31674469  1.24489999 -1.25528395 -2.15396523\n",
      " -1.09670901 -0.74976933 -0.16744931 -1.85072327  2.18610358 -0.05389732\n",
      "  1.03803301  0.33730686 -1.46470749 -1.26404095  0.25509247  0.0622906\n",
      "  0.27852032 -0.52661455  0.8529616   0.58257025 -0.57665855  1.39906311\n",
      "  0.28237963  1.65660369  1.99121034  0.63888913  0.77324259 -1.37577236\n",
      "  0.17209321 -0.2433672   0.63282913  1.48697102  2.34353542 -1.70379281\n",
      "  3.19445586 -1.90496063 -0.51309574  0.79082954 -1.44803131 -0.68631476\n",
      "  0.62008876 -2.34002233 -0.5785594   0.52706939  3.00616074 -1.36615109\n",
      " -2.79532719 -1.17940307 -0.27734265  0.71130925 -0.06620383  0.33663416\n",
      "  0.72049969 -0.92321801 -2.16032648 -0.89048958 -1.41371119 -0.41891441\n",
      "  0.42834592  1.81048751 -1.82744563 -0.26700613  0.7743727   0.80048114\n",
      "  1.1333636   3.27469778 -0.0188297   0.92457372 -0.1246058  -0.58028609\n",
      " -0.01926111  1.05892003 -1.42478561  1.06891561  2.57287121 -1.29488206\n",
      "  0.74771804  1.30669165 -1.32134306  1.6501019  -0.12401557  0.96340084\n",
      "  0.26050946  1.38263357 -0.02877662  2.34315634 -0.26337367  1.91620123\n",
      " -0.77454543  1.73926425  0.08038983 -0.60325927  0.29508227  0.48126751\n",
      "  0.52710861  0.94171894]\n",
      "[-1.7577837  -1.0874279   1.5300866  -0.1115231  -0.37980682  1.4828517\n",
      "  1.3180419   0.11094163  0.7430535  -0.45461136  0.58841336  0.5763913\n",
      "  1.210707    1.3132795  -0.86962503 -0.18507595 -0.47440064  1.5100725\n",
      "  1.0965794   1.0600823  -0.27457932 -0.70003706  2.3117511   1.4944884\n",
      "  0.25560892 -2.866659   -0.28312334  0.34263936 -0.67723423  0.71714777\n",
      "  0.25549442  0.71732044 -0.13262457  0.01792452 -0.3184774   0.5271619\n",
      "  0.7561084  -2.1247065   1.061429   -0.21065854  0.6877343  -1.4956383\n",
      "  0.60346967 -2.6955893   0.37694618 -1.0164185   0.5430663   0.1200121\n",
      " -2.6315718   0.6216742   1.1583976  -2.5385962   1.326312   -0.10284371\n",
      " -0.0286147  -0.9132947   0.7647564   0.79202783 -1.8625957  -0.7418395\n",
      "  0.5884277  -0.9917992  -0.62114453  1.5367815  -0.6628939   0.6712103\n",
      "  0.12914915  0.21228492  0.9017655  -0.25083402  0.71500814  0.08644514\n",
      "  0.59993285  0.5766137   0.64095974  0.47888306 -2.8426213  -2.8502681\n",
      " -0.140544   -1.5917364   0.26691505  0.59476066  0.85868204  1.0322351\n",
      "  0.25671318 -0.34831643  1.752927   -0.21967097 -0.77352476  1.6995213\n",
      "  1.3996491  -0.9419836   0.85996443 -1.8812876  -2.5428605   0.39351937\n",
      " -1.2882805   0.56548136  1.006273    1.2217585   3.5744793   1.717737\n",
      "  1.6917158  -2.2176905  -0.3167447   1.2449     -1.255284   -2.1539652\n",
      " -1.096709   -0.74976933 -0.16744931 -1.8507233   2.1861036  -0.05389732\n",
      "  1.038033    0.33730686 -1.4647075  -1.264041    0.25509247  0.0622906\n",
      "  0.27852032 -0.52661455  0.8529616   0.58257025 -0.57665855  1.3990631\n",
      "  0.28237963  1.6566037   1.9912103   0.63888913  0.7732426  -1.3757724\n",
      "  0.17209321 -0.2433672   0.6328291   1.486971    2.3435354  -1.7037928\n",
      "  3.1944559  -1.9049606  -0.51309574  0.79082954 -1.4480313  -0.68631476\n",
      "  0.62008876 -2.3400223  -0.5785594   0.5270694   3.0061607  -1.3661511\n",
      " -2.7953272  -1.1794031  -0.27734265  0.71130925 -0.06620383  0.33663416\n",
      "  0.7204997  -0.923218   -2.1603265  -0.8904896  -1.4137112  -0.4189144\n",
      "  0.42834592  1.8104875  -1.8274456  -0.26700613  0.7743727   0.80048114\n",
      "  1.1333636   3.2746978  -0.0188297   0.9245737  -0.1246058  -0.5802861\n",
      " -0.01926111  1.05892    -1.4247856   1.0689156   2.5728712  -1.294882\n",
      "  0.74771804  1.3066916  -1.3213431   1.6501019  -0.12401557  0.96340084\n",
      "  0.26050946  1.3826336  -0.02877662  2.3431563  -0.26337367  1.9162012\n",
      " -0.77454543  1.7392642   0.08038983 -0.60325927  0.29508227  0.4812675\n",
      "  0.5271086   0.94171894]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1728/2772009982.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if index_to_word[i] in word2vec:\n",
      "/tmp/ipykernel_1728/2772009982.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  embedding_matrix[i] = word2vec[index_to_word[i]]\n",
      "/tmp/ipykernel_1728/2772009982.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  print(word2vec[index_to_word[4]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 25000\n",
    "word_vector_dim = 200\n",
    "epochs = 4\n",
    "embedding_matrix = np.random.rand(vocab_size,word_vector_dim)\n",
    "#단어사전에 존재하는 단어별 워드 벡터를 복사합니다.\n",
    "for i in range(vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]\n",
    "print(index_to_word[4])        \n",
    "print(embedding_matrix[4])\n",
    "print(word2vec[index_to_word[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fadbb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = tf.keras.Sequential()\n",
    "final_model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))\n",
    "final_model.add(tf.keras.layers.LSTM(256))\n",
    "final_model.add(tf.keras.layers.Dense(16,activation='relu'))\n",
    "final_model.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "468aff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "227/227 [==============================] - 8s 29ms/step - loss: 0.4555 - accuracy: 0.7787 - val_loss: 0.3699 - val_accuracy: 0.8332\n",
      "Epoch 2/4\n",
      "227/227 [==============================] - 6s 28ms/step - loss: 0.3265 - accuracy: 0.8572 - val_loss: 0.3330 - val_accuracy: 0.8544\n",
      "Epoch 3/4\n",
      "227/227 [==============================] - 6s 28ms/step - loss: 0.2700 - accuracy: 0.8881 - val_loss: 0.3291 - val_accuracy: 0.8576\n",
      "Epoch 4/4\n",
      "227/227 [==============================] - 6s 28ms/step - loss: 0.2290 - accuracy: 0.9067 - val_loss: 0.3525 - val_accuracy: 0.8561\n"
     ]
    }
   ],
   "source": [
    "final_model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "final_history = final_model.fit(X_train,\n",
    "                           y_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size = 512,\n",
    "                           validation_data=(X_val,y_val),\n",
    "                           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "509399cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.3558 - accuracy: 0.8544\n",
      "모델의 정확도는 85% 입니다.\n"
     ]
    }
   ],
   "source": [
    "model_loss,model_acc = final_model.evaluate(X_test,y_test,verbose=2)\n",
    "print(\"모델의 정확도는 {}% 입니다.\".format(int(model_acc*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c98a6",
   "metadata": {},
   "source": [
    "## 회고합시다.\n",
    "#### 이번 프로젝트를 진행하며 예상치 못한 오류들을 매우 많이 만났습니다.\n",
    "* load_word2vec_format의 encoding 오류는 Word2Vec.load 함수로 교체하여 해결했습니다.\n",
    "* vocab이라는 속성이 gensim에서 사라졌기 때문에 다운그레이드 하지 않으면 오류가 발생합니다.(pip install --upgrade gensim==3.8.3)\n",
    "\n",
    "#### 성능 향상\n",
    "* train set의 비율을 기존보다 조금 올려보니 약간의 성능 향상을 볼 수 있었습니다.\n",
    "* 반복 학습의 효과가 미미했기 때문에 epoch는 4회가 가장 높은 성능을 보였습니다.\n",
    "* 또, pre trained model을 사용하면 그렇지 않은 model보다 기본적으로 높은 정확도를 노릴 수 있었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
