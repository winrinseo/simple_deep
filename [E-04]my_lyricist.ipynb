{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39a5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#파일을 행단위로 끊어서 저장\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760f9fa",
   "metadata": {},
   "source": [
    "#### 모델이 데이터를 학습할 수 있게 데이터를 전처리하는 과정을 거칩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9605fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장을 사용할 수 있도록 정규화\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697b4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence)==0: continue\n",
    "    if sentence[-1] == \":\" : continue\n",
    "    \n",
    "    p_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(p_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6522beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰 : 텐서에 들어가있는 문자의 인덱스 , 텐서 : 단어의 인덱스로 구성\n",
    "def tokenize(corpus,size = 7000):\n",
    "    #7천개의 단어를 기억할 수 있는 tokenizer를 만듬\n",
    "    #7천 단어에 포함되지 못한 단어는 <unk>로 변경\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = size,\n",
    "        filters = ' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    #corpus를 이용해 단어를 학습시킴\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    #tokenizer를 이용해 corpus를 텐서로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    #입력 데이터의 시퀀스 길이를 일정하게 맞춰줌\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    \n",
    "    return tensor,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddca8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "#단어의 인덱스로 구성된 문자열로 표현된다.\n",
    "tensor,tokenizer = tokenize(corpus)\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d9bd1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "#tokenizer에 저장된 단어\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx >= 10: break\n",
    "# 2번 인덱스가 <start>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751935e",
   "metadata": {},
   "source": [
    "* 텐서에 0으로 저장된 값은 문자가 비어있는것. 0은 \\<pad>로 채워질 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dce8b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "#원본 문장\n",
    "print(tensor[0])\n",
    "#마지막 토큰을 잘라내어 소스 문장을 생성. 마지막 문장은 pad일 가능성이 높음\n",
    "src_input = tensor[:,:-1]\n",
    "\n",
    "print(src_input[0])\n",
    "#start 태그를 잘라내어 타겟 문장을 생성\n",
    "tgt_input = tensor[:,1:]\n",
    "\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb3bf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input)//BATCH_SIZE\n",
    "#Tokenizer가 생성한 단어사전 내 단어 7천개와 <pad>문자 1개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1;\n",
    "\n",
    "#데이터 셋을 만든다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input,tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f96e49",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 끝!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff94ee",
   "metadata": {},
   "source": [
    "#### 단어 생성기 모델 생성\n",
    "###### 단어 생성기 모델의 구조\n",
    "* embedding : 단어를 추상적으로 변환하는 역할을 합니다.\n",
    "* RNN : 문장을 순차적으로 읽으며 단어간의 연관성을 분석합니다.\n",
    "* Linear(Dense) : RNN에서 만들어낸 결과를 바탕으로 생성할 단어를 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df44bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 생성기 클래스 정의\n",
    "class TextGen(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_size,hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size,return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size,return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self,x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de443ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGen(VOCAB_SIZE,embedding_size,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bcf8fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[ 1.25674662e-04,  2.26266537e-04,  2.96517654e-04, ...,\n",
       "          3.63301864e-04, -4.63798679e-05, -3.29888862e-05],\n",
       "        [ 2.16798071e-04,  1.65018166e-04,  4.81989700e-04, ...,\n",
       "          6.58341974e-04,  6.75437987e-05,  1.66632744e-04],\n",
       "        [ 1.68769766e-04,  9.84599028e-05,  6.56000338e-04, ...,\n",
       "          6.67901186e-04,  4.69546540e-05, -8.93163742e-05],\n",
       "        ...,\n",
       "        [-6.27967878e-04,  1.34634087e-03, -2.42275349e-03, ...,\n",
       "          1.42183353e-03,  1.77932973e-03, -5.98388375e-04],\n",
       "        [-1.05585018e-03,  1.83710374e-03, -2.82890140e-03, ...,\n",
       "          1.39157264e-03,  2.02784571e-03, -7.96724344e-04],\n",
       "        [-1.43268018e-03,  2.27780477e-03, -3.19208903e-03, ...,\n",
       "          1.36125286e-03,  2.23818817e-03, -9.99430777e-04]],\n",
       "\n",
       "       [[ 1.25674662e-04,  2.26266537e-04,  2.96517654e-04, ...,\n",
       "          3.63301864e-04, -4.63798679e-05, -3.29888862e-05],\n",
       "        [-2.02149138e-04,  7.72706990e-05,  6.77507720e-04, ...,\n",
       "          5.96968806e-04, -1.88765902e-04, -7.76675370e-05],\n",
       "        [-6.00270068e-05,  1.58834482e-05,  9.15765006e-04, ...,\n",
       "          2.31776459e-04, -3.08617193e-04, -4.96883004e-04],\n",
       "        ...,\n",
       "        [-1.45335356e-03,  3.00214090e-03, -2.31212564e-03, ...,\n",
       "          7.43409444e-04,  1.38714269e-03, -1.46330660e-03],\n",
       "        [-1.65352400e-03,  3.27041186e-03, -2.71006185e-03, ...,\n",
       "          7.98891473e-04,  1.63648161e-03, -1.60222792e-03],\n",
       "        [-1.84695097e-03,  3.51117505e-03, -3.06496420e-03, ...,\n",
       "          8.52019468e-04,  1.84382778e-03, -1.73564791e-03]],\n",
       "\n",
       "       [[ 1.25674662e-04,  2.26266537e-04,  2.96517654e-04, ...,\n",
       "          3.63301864e-04, -4.63798679e-05, -3.29888862e-05],\n",
       "        [ 1.15028241e-04,  5.29740762e-04,  5.61034482e-04, ...,\n",
       "          7.64159427e-04, -1.27149151e-05, -3.45985136e-05],\n",
       "        [-1.27784282e-04,  5.14352578e-04,  5.14842395e-04, ...,\n",
       "          1.07339886e-03,  2.11835533e-04, -1.18040487e-04],\n",
       "        ...,\n",
       "        [-6.62354345e-04,  2.45873327e-03, -2.43591936e-03, ...,\n",
       "          6.56370830e-04,  9.96249262e-04, -6.73144241e-04],\n",
       "        [-1.04459806e-03,  2.68615945e-03, -2.70788511e-03, ...,\n",
       "          7.48232065e-04,  1.36323040e-03, -9.18822596e-04],\n",
       "        [-1.38497981e-03,  2.91302055e-03, -2.97137583e-03, ...,\n",
       "          8.39307322e-04,  1.69106107e-03, -1.15610601e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.25674662e-04,  2.26266537e-04,  2.96517654e-04, ...,\n",
       "          3.63301864e-04, -4.63798679e-05, -3.29888862e-05],\n",
       "        [ 1.94719803e-04,  2.96185171e-04,  4.34548798e-04, ...,\n",
       "          2.39897257e-04, -1.77546273e-04,  1.08427696e-04],\n",
       "        [ 4.18770942e-04,  1.35127237e-04,  2.15348919e-04, ...,\n",
       "          3.54900752e-04, -1.65193080e-04,  4.98544832e-05],\n",
       "        ...,\n",
       "        [-8.70310643e-04,  2.44146999e-04, -8.88411538e-04, ...,\n",
       "          1.45594764e-03,  1.75632886e-03, -4.86413774e-04],\n",
       "        [-1.26338843e-03,  7.11264147e-04, -1.19877339e-03, ...,\n",
       "          1.42126507e-03,  1.92134443e-03, -6.76553580e-04],\n",
       "        [-1.61107839e-03,  1.17534620e-03, -1.52738765e-03, ...,\n",
       "          1.39598758e-03,  2.08751485e-03, -8.69922165e-04]],\n",
       "\n",
       "       [[ 1.25674662e-04,  2.26266537e-04,  2.96517654e-04, ...,\n",
       "          3.63301864e-04, -4.63798679e-05, -3.29888862e-05],\n",
       "        [ 2.35597341e-04, -1.34913294e-04,  1.95819157e-04, ...,\n",
       "          7.46684615e-04,  8.14688392e-05, -1.54734240e-04],\n",
       "        [ 2.82904046e-04, -6.08747883e-04,  4.06746811e-04, ...,\n",
       "          9.01770312e-04,  1.97686764e-04, -2.49969919e-04],\n",
       "        ...,\n",
       "        [-1.83885195e-03,  2.58339779e-03, -3.07544903e-03, ...,\n",
       "          4.68562968e-04,  2.43077590e-03, -9.82015510e-04],\n",
       "        [-2.03508371e-03,  2.93808780e-03, -3.34938825e-03, ...,\n",
       "          5.54971513e-04,  2.55474844e-03, -1.19896303e-03],\n",
       "        [-2.20569060e-03,  3.25330626e-03, -3.59417568e-03, ...,\n",
       "          6.35614968e-04,  2.63948180e-03, -1.40159379e-03]],\n",
       "\n",
       "       [[ 1.25674662e-04,  2.26266537e-04,  2.96517654e-04, ...,\n",
       "          3.63301864e-04, -4.63798679e-05, -3.29888862e-05],\n",
       "        [ 5.67353527e-05,  1.86525780e-04,  5.94574492e-04, ...,\n",
       "          4.35863360e-04,  2.79597240e-04,  2.50747362e-05],\n",
       "        [ 2.35234475e-04,  1.33519774e-04,  5.17011562e-04, ...,\n",
       "          7.36746355e-04,  3.92306334e-04,  4.69461927e-04],\n",
       "        ...,\n",
       "        [-1.20222277e-03,  8.65285809e-04, -2.11845408e-03, ...,\n",
       "          1.18909415e-03,  5.00646944e-04,  5.45918301e-04],\n",
       "        [-1.50964700e-03,  1.35606690e-03, -2.51441938e-03, ...,\n",
       "          1.16367312e-03,  8.67196766e-04,  2.71576340e-04],\n",
       "        [-1.77692110e-03,  1.82012597e-03, -2.87045212e-03, ...,\n",
       "          1.14513491e-03,  1.20472908e-03, -1.48710342e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "#배치 하나를 모델에 넣어본다.\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f1c5e",
   "metadata": {},
   "source": [
    "#### 모델 shape가 (256,20,7001)로 구성되어 있는데..\n",
    "* 7001은 Dense 레이어의 출력 차원 수이며 7001개의 단어의 확률 분포입니다.\n",
    "* 256은 설정한 배치 사이즈입니다. 하나의 배치에 256개의 문장 데이터를 가지도록 설정한것입니다.\n",
    "* 20은 문장 시퀀스의 길이입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "106dcedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"optimizer = tf.keras.optimizers.Adam()\\nloss = tf.keras.losses.SparseCategoricalCrossentropy(\\n    from_logits = True,\\n    reduction='none'\\n)\\n#모델 학습\\nmodel.compile(loss=loss, optimizer=optimizer)\\nmodel.fit(dataset,epochs=30)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True,\n",
    "    reduction='none'\n",
    ")\n",
    "#모델 학습\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset,epochs=30)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf2e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,init_sentence = \"<start>\",max_len = 20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input,dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        #입력받은 문장의 텐서를 입력\n",
    "        predict = model(test_tensor)\n",
    "        #가장 높은 확률을 가지는 word_idx를 뽑아냄\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict,axis=-1),axis=-1)[:,-1]\n",
    "        #예즉된 word_index를 문장 뒤에 붙임\n",
    "        test_tensor = tf.concat([test_tensor,tf.expand_dims(predict_word,axis=0)],axis=-1)\n",
    "        #종료조건\n",
    "        if predict_word.numpy()[0]==end_token or test_tensor.shape[1] >= max_len: \n",
    "            break\n",
    "        \n",
    "    \n",
    "    generated = \"\"\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index]+\" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448071c",
   "metadata": {},
   "source": [
    "#### 문장을 생성해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b04092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he bisson bisson minion studies became bruised bruised bruised warriors shame shame shame spoons portion runn runn rod rod \n",
      "<start> i soundly making spotted spotted spotted captives captives captives captives captives sinew sinew seat mother vaward vaward vaward vaward \n"
     ]
    }
   ],
   "source": [
    "sentence = generate_text(model, tokenizer, init_sentence=\"<start> he\")\n",
    "print(sentence)\n",
    "sentence = generate_text(model, tokenizer, init_sentence=\"<start> i\")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13022354",
   "metadata": {},
   "source": [
    "## 인공지능 작사가 만들기!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d90eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7de18fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정제\n",
    "corpos = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence)==0: continue\n",
    "    if sentence[-1] == \":\" : continue\n",
    "    \n",
    "    p_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(p_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ee6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor,tokenizer = tokenize(corpus,18000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31c14116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  175   22 2151  320 1553    4  173   12  386   15    3    0    0\n",
      "    0]\n",
      "[ 175   22 2151  320 1553    4  173   12  386   15    3    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "#너무 긴 문장은 과도한 pad를 가지므로 잘라준다\n",
    "src_input = tensor[:,:-1]\n",
    "src_input = src_input[:,:15]\n",
    "print(src_input[0])\n",
    "tgt_input = tensor[:,1:]\n",
    "tgt_input = tgt_input[:,:15]\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ecff500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42661ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159811, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#테스트 데이터 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
    "    src_input, \n",
    "    tgt_input, \n",
    "    test_size=0.2, \n",
    "    shuffle=True, \n",
    "    random_state=34)\n",
    "enc_train.shape\n",
    "#enc_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e61f1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 15), (256, 15)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train)//BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1;\n",
    "\n",
    "#데이터 셋을 만든다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train,dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abec8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "hidden_size = 1500\n",
    "lyricist = TextGen(VOCAB_SIZE,embedding_size,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dda31d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7bbd824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "624/624 [==============================] - 311s 486ms/step - loss: 3.4640 - val_loss: 3.1349\n",
      "Epoch 2/10\n",
      "624/624 [==============================] - 303s 485ms/step - loss: 2.9908 - val_loss: 2.9316\n",
      "Epoch 3/10\n",
      "624/624 [==============================] - 303s 486ms/step - loss: 2.7766 - val_loss: 2.8060\n",
      "Epoch 4/10\n",
      "624/624 [==============================] - 303s 486ms/step - loss: 2.5978 - val_loss: 2.7172\n",
      "Epoch 5/10\n",
      "624/624 [==============================] - 304s 487ms/step - loss: 2.4327 - val_loss: 2.6502\n",
      "Epoch 6/10\n",
      "624/624 [==============================] - 303s 486ms/step - loss: 2.2780 - val_loss: 2.5992\n",
      "Epoch 7/10\n",
      "624/624 [==============================] - 304s 487ms/step - loss: 2.1307 - val_loss: 2.5594\n",
      "Epoch 8/10\n",
      "624/624 [==============================] - 305s 488ms/step - loss: 1.9892 - val_loss: 2.5313\n",
      "Epoch 9/10\n",
      "624/624 [==============================] - 304s 488ms/step - loss: 1.8542 - val_loss: 2.5163\n",
      "Epoch 10/10\n",
      "624/624 [==============================] - 304s 487ms/step - loss: 1.7265 - val_loss: 2.5112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee59130130>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True,\n",
    "    reduction='none'\n",
    ")\n",
    "#모델 학습\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "lyricist.fit(dataset,epochs=10,validation_data=(enc_val,dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6ce456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s got a mag with the <unk> <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> he\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4433015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> my name is <end> ',\n",
       " '<start> she s got me runnin round and round <end> ',\n",
       " '<start> may you arrive and it s not there <end> ',\n",
       " '<start> he s got a mag with the <unk> <end> ',\n",
       " '<start> love you , love you <end> ',\n",
       " '<start> me and my homies , so drop that <end> ',\n",
       " '<start> to the <unk> of the lamb <end> ',\n",
       " '<start> no , no , no , no , no , no , no , ',\n",
       " '<start> by the way you see me <end> ',\n",
       " '<start> real nixgaz do real things on the road to riches and diamond rings <end> ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10소절만 만들어보자\n",
    "list = [\"my\",\"she\",\"may\",\"he\",\"love\",\"me\",\"to\",\"no\",\"by\",\"real\"]\n",
    "gen = []\n",
    "for word in list:\n",
    "    sentence = generate_text(lyricist, tokenizer, init_sentence=\"<start> \"+word, max_len=15)\n",
    "    gen.append(sentence)\n",
    "\n",
    "gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a67fa8",
   "metadata": {},
   "source": [
    "## 회고합시다.\n",
    "* 동작 자체는 나쁘지않게 동작하는것 같은데 노래 가사라고 생각하면 봐줄만한 정도인것 같습니다.\n",
    "* 다만 학습 데이터가 매우 많아 학습에 너무 시간이 오래걸려 모델의 적절한 사이즈를 찾는게 다소 어려웠습니다.  \n",
    "* 모델의 검증 loss를 2.2 이하로 내려보고싶었지만 아무리 맞춰봐도 2.3이상으론 줄어들지 않아서 적당히 타협할수밖에 없었습니다.(아예 size를 대폭 올렸더니 시간만 5시간이 걸린데다가 loss도 거기서 거기..)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
